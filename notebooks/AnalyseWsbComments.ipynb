{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdafdee1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be57434c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T23:23:59.619280Z",
     "start_time": "2021-08-25T23:23:56.250746Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values, execute_batch\n",
    "\n",
    "sys.path.append(\"C:\\\\Users\\\\SHIRAM\\\\Documents\\\\WSB\\\\\")\n",
    "from generate_urls import get_all_tickers\n",
    "from reddit_stuff import get_conn, get_sqlalchemy_engine, get_reddit_client_praw\n",
    "\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from utils import batch\n",
    "from wsb import Gather\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "gather = Gather()\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5445cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### What do we have online?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6985cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with gather.get_psycopg2_conn() as conn: \n",
    "    with conn.cursor() as cur: \n",
    "        cur.execute(\"SELECT * FROM wsb_comments_temp;\")\n",
    "        wsb_comments_analytics_df = pd.DataFrame.from_records(\n",
    "            cur.fetchall(), \n",
    "            columns=[\"created_utc\", \"submission_id\", \"comment_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571a818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-14T09:42:00.714671Z",
     "start_time": "2021-08-14T09:31:45.713454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with gather.get_psycopg2_conn() as conn: \n",
    "    with conn.cursor() as cur: \n",
    "        cur.execute(\"SELECT created_utc, submission_id, comment_id FROM wsb_comments_analytics;\")\n",
    "        wsb_comments_analytics_df = pd.DataFrame.from_records(\n",
    "            cur.fetchall(), \n",
    "            columns=[\"created_utc\", \"submission_id\", \"comment_id\"]\n",
    "        )\n",
    "\n",
    "wsb_comments_aggs_df = wsb_comments_analytics_df.groupby([wsb_comments_analytics_df[\"created_utc\"].dt.date, \"submission_id\"])[\"comment_id\"].nunique()\n",
    "wsb_comments_aggs_df = pd.DataFrame(wsb_comments_aggs_df)\n",
    "wsb_comments_aggs_df.columns = [\"comments_nunique\"]\n",
    "wsb_comments_aggs_df = wsb_comments_aggs_df.reset_index()\n",
    "wsb_comments_aggs_df.loc[:, \"created_utc\"] = pd.to_datetime(wsb_comments_aggs_df[\"created_utc\"])\n",
    "wsb_comments_aggs_df = wsb_comments_aggs_df.groupby([wsb_comments_aggs_df.created_utc]).apply(lambda x: x.sort_values(\"comments_nunique\", ascending=False).max() )\n",
    "wsb_comments_aggs_df = wsb_comments_aggs_df.drop(columns=[\"created_utc\"])\n",
    "wsb_comments_aggs_df.to_csv(\"wsb_comments_aggs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e261074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T07:10:31.235110Z",
     "start_time": "2021-08-15T07:10:31.053111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if \"wsb_comments_aggs_df\" not in locals(): \n",
    "    wsb_comments_aggs_df = pd.read_csv(\"wsb_comments_aggs_df.csv\", index_col=0)\n",
    "    wsb_comments_aggs_df.index = pd.to_datetime(wsb_comments_aggs_df.index)\n",
    "    \n",
    "wsb_comments_aggs_df[\"comments_nunique\"].plot(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99bd901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T07:10:32.350916Z",
     "start_time": "2021-08-15T07:10:32.267812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Where are the gaps?\n",
    "all_dates_in_db = wsb_comments_aggs_df.index.tolist()\n",
    "all_bdates = pd.bdate_range(start=wsb_comments_aggs_df.index.min(), end=wsb_comments_aggs_df.index.max(), freq=\"1D\")\n",
    "dates_not_found_in_db = [ele for ele in all_bdates if ele not in all_dates_in_db]\n",
    "dates_not_found_in_db = sorted(dates_not_found_in_db)\n",
    "missing_dates_arr = np.array(dates_not_found_in_db) \n",
    "missing_start_end_date_pairs = [(int(start.timestamp()), int(end.timestamp()) ) for start, end in zip(missing_dates_arr, np.roll(missing_dates_arr, -1)[:-1] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec93984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T07:10:35.072366Z",
     "start_time": "2021-08-15T07:10:34.838369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "saved_filename = \"all_saved_responses_before_sleep_only_stickied.json\"\n",
    "\n",
    "if os.path.isfile(saved_filename): \n",
    "    with open(saved_filename, \"r\") as f: \n",
    "        all_responses = json.load(f)\n",
    "else:\n",
    "    all_responses = []\n",
    "    for start, end in tqdm(missing_start_end_date_pairs):\n",
    "        host = \"https://api.pushshift.io/reddit/search/submission/?\"\n",
    "        params = {\"after\": f\"{start}\", \"before\": f\"{end}\", \"subreddit\": \"wallstreetbets\", \"stickied\": \"true\"}\n",
    "        response = requests.get(host, params)\n",
    "        if response.status_code == 200: \n",
    "            all_responses += response.json()[\"data\"]\n",
    "    # write this shit down to a file\n",
    "    with open(saved_filename, \"w\") as f: \n",
    "        json.dump(all_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c227f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T07:10:36.721361Z",
     "start_time": "2021-08-15T07:10:35.927353Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "automod_results = [ele for ele in all_responses if \"author\" in ele.keys() and ele[\"author\"] == \"AutoModerator\"]\n",
    "automod_df = pd.DataFrame([{\"id\": ele[\"id\"], \"created_utc\": ele[\"created_utc\"], \"num_comments\": ele[\"num_comments\"], \"url\": ele[\"url\"]} for ele in all_responses])\n",
    "automod_df.loc[:, \"created_utc\"] = automod_df[\"created_utc\"].apply(lambda x: pd.to_datetime(x, unit=\"s\"))\n",
    "automod_df = automod_df.set_index(\"created_utc\")\n",
    "\n",
    "automod_df_counts = automod_df.groupby(automod_df.index.date).aggregate({\"num_comments\": \"sum\"})\n",
    "automod_df_counts.index.name = \"created_utc\"\n",
    "automod_df_counts.index = pd.to_datetime(automod_df_counts.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "wsb_comments_aggs_df[\"comments_nunique\"].plot(ax=ax, label=\"Old\")\n",
    "automod_df_counts[\"num_comments\"].plot(ax=ax, label=\"New\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d31f5",
   "metadata": {},
   "source": [
    "### Gather all comments from those new submissions and insert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f51cc4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T07:29:00.702449Z",
     "start_time": "2021-08-26T07:18:14.262819Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "all_comments_filename = \"../data/all_comments_arr_2021_08_17.json\"\n",
    "all_missing_comments_filename = \"../data/all_missing_comment_ids.json\"\n",
    "\n",
    "# if the file is \n",
    "if \"all_comments_arr\" not in locals() and os.path.isfile(all_comments_filename):\n",
    "    with open(all_comments_filename, \"r\") as f: \n",
    "        all_comments_arr = json.load(f)\n",
    "        \n",
    "    # if this is present, then all missing comment ids must also be present\n",
    "    if \"all_missing_comment_ids\" not in locals() and os.path.isfile(all_missing_comments_filename): \n",
    "        with open(all_missing_comments_filename, \"r\") as f: \n",
    "            all_missing_comment_ids = json.load(f)\n",
    "\n",
    "else:\n",
    "    if \"all_missing_comment_ids\" not in locals() and os.path.isfile(\"all_missing_comment_ids.json\"): \n",
    "        with open(\"all_missing_comment_ids.json\", \"r\") as f: \n",
    "            all_missing_comment_ids = json.load(f)\n",
    "    else:\n",
    "        hosts = [f\"https://api.pushshift.io/reddit/submission/comment_ids/{ele['id']}\" for ele in all_responses]\n",
    "        all_missing_comment_ids = gather.get_all_comments_from_pmaw_single_thread(submissions=all_responses)\n",
    "        with open(\"all_missing_comment_ids.json\", \"w\") as f:\n",
    "            json.dump(all_missing_comment_ids, f)\n",
    "\n",
    "    all_missing_comments_only = []\n",
    "    for ele in all_missing_comment_ids:\n",
    "        all_missing_comments_only += ele[\"comment_ids\"]\n",
    "\n",
    "    batch_len = 600\n",
    "    missing_comments_batched = batch(all_missing_comments_only, n=batch_len)\n",
    "\n",
    "    all_comments_arr = []\n",
    "    for missing_comments in tqdm(missing_comments_batched, total=len(all_missing_comments_only)//batch_len):\n",
    "        comments = gather.pmaw_api.search_comments(ids=missing_comments)\n",
    "        all_comments_arr += comments\n",
    "    \n",
    "    with open(\"all_comments_arr_2021_08_17.json\", \"w\") as f:\n",
    "        json.dump(all_comments_arr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c760637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T07:59:03.022931Z",
     "start_time": "2021-08-26T07:51:12.161497Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# clean up comment submissions\n",
    "comment_submission_ids = pd.DataFrame(all_missing_comment_ids).explode(\"comment_ids\")[[\"comment_ids\", \"submission_id\"]]#.reset_index(drop=True).to_dict()\n",
    "comment_submission_ids.columns = [\"id\", \"submission_id\"]\n",
    "comment_submission_ids.set_index(\"id\", inplace=True)\n",
    "\n",
    "# merge all_comments_df with the comment_submissions, and convert it to a list with dict type\n",
    "all_comments_df = pd.DataFrame.from_dict(all_comments_arr)\n",
    "all_comments_df.set_index(\"id\", inplace=True)\n",
    "all_comments_df = pd.merge(all_comments_df, comment_submission_ids, left_index=True, right_index=True)\n",
    "all_comments_df = all_comments_df.reset_index()\n",
    "\n",
    "# all_comments_df.to_csv(\"all_comments_with_sub_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddb85d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T08:00:21.065296Z",
     "start_time": "2021-08-26T07:59:37.328826Z"
    }
   },
   "outputs": [],
   "source": [
    "all_comments_df = all_comments_df.drop_duplicates(inplace=False, subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bac947f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T08:16:44.996509Z",
     "start_time": "2021-08-26T08:16:42.287473Z"
    }
   },
   "outputs": [],
   "source": [
    "comments_columns = [\"created_utc\", \"retrieved_on\", \"id\", \"parent_id\", \"link_id\", \"author\", \"submission_id\", \"body\", \"subreddit\"]\n",
    "all_comments_df = all_comments_df[comments_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb80e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T23:38:31.169372Z",
     "start_time": "2021-08-25T23:38:31.074753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gather.insert_all_comments_to_db_using_df_pmaw(df=all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4247e666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T08:16:53.023719Z",
     "start_time": "2021-08-26T08:16:52.925716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatting into correct format...\n"
     ]
    }
   ],
   "source": [
    "print(\"formatting into correct format...\")\n",
    "comments_cols = []\n",
    "to_insert_comments = [\"created_utc\", \"retrieved_on\", \"id\", \"parent_id\", \"link_id\", \"author\", \"author_fullname\", \"submission_id\", \"body\", \"subreddit\"]\n",
    "\n",
    "# make a reasonable batch len\n",
    "batch_len = 50000\n",
    "\n",
    "# there are too many comments, batch them\n",
    "batched_comments = batch(all_comments_arr, n=batch_len)\n",
    "\n",
    "# we need to pre-calculate the total number of batches here\n",
    "total_batches = int(len(all_comments_df) // batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5a5f00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T12:07:25.643920Z",
     "start_time": "2021-08-26T12:06:59.080918Z"
    }
   },
   "outputs": [],
   "source": [
    "all_comments_df = all_comments_df.drop_duplicates(subset=[\"created_utc\", \"id\"], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9fa4f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T12:39:03.517692Z",
     "start_time": "2021-08-26T12:31:26.486729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Copy-ing into the db...\n",
      "-- Done\n"
     ]
    }
   ],
   "source": [
    "text_stream = StringIO()\n",
    "all_comments_df.to_csv(text_stream, header=True, index=False)\n",
    "text_stream.seek(0)\n",
    "\n",
    "print(\"-- Copy-ing into the db...\")\n",
    "focused_cols = \",\".join(all_comments_df.columns.tolist())\n",
    "with gather.get_psycopg2_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.copy_expert(\n",
    "                sql=f\"\"\"copy comments({focused_cols}) from stdin with (format csv, delimiter ',', header);\"\"\",\n",
    "                file=text_stream,\n",
    "            )\n",
    "            conn.commit()\n",
    "        except psycopg2.programmingerror as e:\n",
    "            print(e)\n",
    "print(\"-- Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccc487",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1188269c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T07:52:45.114902Z",
     "start_time": "2021-08-18T07:48:19.642629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_comments_df = pd.read_json(\"all_comments_arr_2021_08_17.json\", lines=True, chunksize=10000)\n",
    "# all_comments_df = pd.read_json(\"all_comments_arr_2021_08_17.json\")\n",
    "# gather.insert_all_comments_to_db_pmaw(df=all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e8cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T06:39:27.831277Z",
     "start_time": "2021-08-15T06:39:27.829237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_sub_id_with_comments = []\n",
    "# for submission_comment_ids in tqdm(all_missing_comment_ids):\n",
    "#     for sub_id, comments in submission_comment_ids.items():\n",
    "#         comments_arr = gather.pmaw_api.search_comments(ids=comments)\n",
    "#         for c in comments_arr:\n",
    "#             c[\"submission_id\"] = sub_id\n",
    "#             all_sub_id_with_comments.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20798f1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-14T09:58:21.181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"all_sub_id_with_comments.json\", \"w\") as f: \n",
    "    json.dump(all_sub_id_with_comments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021f360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T07:44:29.203207Z",
     "start_time": "2021-07-19T07:44:25.265379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "host_v2 = \"https://api.pushshift.io/reddit/search/comment/?\"\n",
    "\n",
    "params_v2 = {\"q\": \"MVIS\", \"subreddit\": \"wallstreetbets\", \"before\": \"60d\"}\n",
    "# params_v2 = {\"q\": \"MVIS\", \"subreddit\": \"wallstreetbets\", \"after\": int(missing_dates_arr[0].timestamp()), \"before\": int(missing_dates_arr[-1].timestamp())}\n",
    "\n",
    "params_v21 = {\"q\": \"Microvision\", \"subreddit\": \"wallstreetbets\", \"before\": \"60d\"}\n",
    "# params_v21 = {\"q\": \"Microvision\", \"subreddit\": \"wallstreetbets\", \"after\": int(missing_dates_arr[0].timestamp()), \"before\": int(missing_dates_arr[-1].timestamp())}\n",
    "\n",
    "params_v22 = {\"q\": \"mvis\", \"subreddit\": \"wallstreetbets\", \"before\": \"60d\"}\n",
    "# params_v22 = {\"q\": \"mvis\", \"subreddit\": \"wallstreetbets\", \"after\": int(missing_dates_arr[0].timestamp()), \"before\": int(missing_dates_arr[-1].timestamp())}\n",
    "\n",
    "all_responses_v2 = []\n",
    "for param in tqdm([params_v2, params_v21, params_v22]):\n",
    "    response_v2 = requests.get(host_v2, params=param)\n",
    "\n",
    "    if response_v2.status_code == 200:\n",
    "        response_v2_json = response_v2.json()\n",
    "        all_responses_v2 += response_v2_json[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f88f4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### What does it say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fab0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.894997Z",
     "start_time": "2021-07-17T19:50:01.719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wsb_com_df = pd.read_csv(\"polygonio_public_wsb_comments.csv\")\n",
    "\n",
    "wsb_com_df.loc[~pd.isna(wsb_com_df[colname]), colname] = \\\n",
    "    wsb_com_df.loc[~pd.isna(wsb_com_df[colname]), colname].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "    \n",
    "wsb_com_df.loc[:, colname] = pd.to_datetime(wsb_com_df[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f760979",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get all tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e073b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.898982Z",
     "start_time": "2021-07-17T19:50:01.770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tickers = get_all_tickers(active=\"true\")\n",
    "tickers_only = [ticker[\"ticker\"].lower() for ticker in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb96ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.898982Z",
     "start_time": "2021-07-17T19:50:01.773Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get data from the comments db\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur: \n",
    "        query = \"SELECT created_utc, comment_id, body FROM wsb_comments_analytics;\"\n",
    "        cur.execute(query)\n",
    "        res = cur.fetchall()\n",
    "\n",
    "wsb_comments_analytics_df = pd.DataFrame.from_records(res)\n",
    "wsb_comments_analytics_df.columns = [\"datetime\", \"comment_id\", \"body\"]\n",
    "wsb_comments_analytics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51743dec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.900982Z",
     "start_time": "2021-07-17T19:50:01.774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "if \"clean_text\" in locals():\n",
    "    del clean_text\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text_arr = text.replace(\":\", \"\").replace(\";\", \"\").lower().split(\" \")\n",
    "    return text_arr\n",
    "\n",
    "def tokenize(doc):\n",
    "    token_list = {str(token).lower(): token.pos_ for token in doc}\n",
    "    return token_list\n",
    "\n",
    "def complex_tkr_condition(tkr, comment_split, comment_split_pos):\n",
    "    if (tkr not in [\"HAS\", \"A\", \"FOR\", \"NEW\", \"ME\", \"CAN\", \"HOW\", \"GO\", \"IF\", \"WHEN\", \"THERE\"]) and (tkr in comment_split_pos.keys()):\n",
    "        if (tkr in comment_split) and (comment_split_pos[tkr] in [\"PROPN\", \"NOUN\"]):   \n",
    "            return tkr\n",
    "        else:\n",
    "            return \"notickers\"\n",
    "    \n",
    "def process_chunk(texts, tickers):\n",
    "    processed_pipe = []\n",
    "    texts = [clean_text(text) for text in texts]\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        tokenized_doc = tokenize(doc)\n",
    "        tickers_found = [tkr for tkr in tickers if complex_tkr_condition(tkr, tokenized_doc)]\n",
    "        processed_pipe.append(tickers_found)\n",
    "    return processed_pipe\n",
    "    \n",
    "def parallel_nlp(texts, tickers, total_len, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"threads\")\n",
    "    tasks = (delayed(process_chunk)(chunk, tickers) for chunk in tqdm(chunker(texts, total_len, chunksize=chunksize)))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n",
    "\n",
    "def update_wsb_analytics(conn, all_tickers_found, all_comment_ids):\n",
    "    with conn.cursor() as cur: \n",
    "        cur.execute(\"PREPARE updateStmt as UPDATE wsb_comments_analytics SET tickers_found=$1 WHERE comment_id=$2\")\n",
    "        execute_batch(cur, \n",
    "                      \"EXECUTE updateStmt (%(tickers_found)s, %(comment_id)s\", \n",
    "                      all_tickers_found, \n",
    "                      all_comment_ids, \n",
    "                      page_size=100)\n",
    "        cur.execute(\"DEALLOCATE updateStmt\")\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbe92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.900982Z",
     "start_time": "2021-07-17T19:50:01.777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if \"comm_id_ticker_info\" in locals():\n",
    "    existing_keys = [list(c.keys())[0] for c in comm_id_ticker_info]\n",
    "    df_ = wsb_comments_analytics_df.loc[~wsb_comments_analytics_df[\"comment_id\"].isin(existing_keys), [\"comment_id\", \"body\"]].values\n",
    "else:\n",
    "    comm_id_ticker_info = []\n",
    "    df_ = wsb_comments_analytics_df[[\"comment_id\", \"body\"]].values\n",
    "    \n",
    "with get_conn() as conn:\n",
    "    prepared_statements = []\n",
    "    for (comment_id, comment_body) in tqdm(df_):\n",
    "        d = {\"comment_id\": comment_id, \"tickers\": []}\n",
    "        try:\n",
    "            comment_text = clean_text(text=comment_body)\n",
    "            comment_split_pos = tokenize(doc=nlp(comment_body))\n",
    "            d[\"tickers\"] = [tkr for tkr in tickers_only if complex_tkr_condition(tkr, comment_text, comment_split_pos)]\n",
    "            if d[\"tickers\"]:\n",
    "                prepared_statements.append(d)\n",
    "        except TypeError as e:\n",
    "            pass\n",
    "        \n",
    "\n",
    "prepared_statements_df = pd.DataFrame(prepared_statements)\n",
    "\n",
    "null_mask = ~pd.isna(prepared_statements_df[\"tickers\"])\n",
    "prepared_statements_df.loc[null_mask, \"tickers\"] = prepared_statements_df.loc[null_mask, \"tickers\"].progress_apply(lambda x: ', '.join(ast.literal_eval(x)))\n",
    "\n",
    "prepared_statements_df.to_csv(\"prepared_statements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd7f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.901982Z",
     "start_time": "2021-07-17T19:50:01.780Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_prepared_statements(comment_df: pd.DataFrame): \n",
    "    with get_conn() as conn: \n",
    "        with conn.cursor() as cur: \n",
    "            cur.execute(\"\"\"SELECT tbl1.id FROM wsb_comments tbl1 WHERE NOT EXISTS (SELECT FROM wsb_comment_analytics tbl2 WHERE tbl2.comment_id = tbl1.id);\"\"\")\n",
    "        \n",
    "    prepared_statements = []\n",
    "    for (comment_id, comment_body) in tqdm(comment_df):\n",
    "        d = {\"comment_id\": comment_id, \"tickers\": []}\n",
    "        try:\n",
    "            comment_text = clean_text(text=comment_body)\n",
    "            comment_split_pos = tokenize(doc=nlp(comment_body))\n",
    "            d[\"tickers\"] = [tkr for tkr in tickers_only if complex_tkr_condition(tkr, comment_text, comment_split_pos)]\n",
    "            if d[\"tickers\"]:\n",
    "                prepared_statements.append(d)\n",
    "        except TypeError as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    prepared_statements_df = pd.DataFrame(prepared_statements)\n",
    "\n",
    "    null_mask = ~pd.isna(prepared_statements_df[\"tickers\"])\n",
    "    prepared_statements_df.loc[null_mask, \"tickers\"] = prepared_statements_df.loc[null_mask, \"tickers\"].progress_apply(lambda x: ', '.join(ast.literal_eval(x)))\n",
    "\n",
    "    prepared_statements_df.to_csv(\"prepared_statements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f9b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.902982Z",
     "start_time": "2021-07-17T19:50:01.781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prepared_statements_df = pd.read_csv(\"prepared_statements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0040b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.903982Z",
     "start_time": "2021-07-17T19:50:01.784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stage 1 \n",
    "print(\"-- Start\")\n",
    "filepath = r\"./prepared_statements.csv\"\n",
    "psql = r\"/usr/lib/postgresql/13/psql\"\n",
    "psql_win = r\"C:\\\\Program Files\\\\PostgreSQL\\\\bin\\\\psql.exe\"\n",
    "\n",
    "db_conn = \"postgresql://postgres:rogerthat@localhost:5433/polygonio\"\n",
    "command_1 = [psql_win, '--command', \"\\copy wsb_comments_analytics_smaller_temp FROM './prepared_statements.csv' WITH (FORMAT csv, header)\", '-d', db_conn]\n",
    "\n",
    "process = Popen(command_1, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "\n",
    "# Print whatever the shell would normally display.\n",
    "stdout = process.communicate()[0].decode('utf-8').strip()\n",
    "print(stdout)\n",
    "\n",
    "# # Running command 2.\n",
    "# process = Popen(command2, stdout = PIPE, stderr = PIPE)\n",
    "\n",
    "# stdout = process.communicate()[0].decode('utf-8').strip()\n",
    "# print(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e28909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.903982Z",
     "start_time": "2021-07-17T19:50:01.786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stage 2 \n",
    "create_wsb_comments_analytics_temp = \\\n",
    "\"\"\"DROP TABLE IF EXISTS esb_comments_analytics_temp;\n",
    "    CREATE TABLE wsb_comments_analytics_temp AS (\n",
    "        SELECT t1.created_utc, t1.submission_id, t1.parent_id, t1.comment_id, t1.author, t1.body, t2.tickers FROM wsb_comments_analytics t1\n",
    "        LEFT JOIN\n",
    "        wsb_comments_analytics_tickers_found t2 on t1.comment_id = t2.comment_id);\"\"\"\"\n",
    "\n",
    "# stage 3\n",
    "truncate_and_drop_and_rename_wsb_comments_analytics = \\\n",
    "\"\"\"TRUNCATE wsb_comments_analytics;DROP TABLE wsb_comments_analytics;ALTER TABLE wsb_comments_analytics_temp RENAME TO wsb_comments_analytics;\"\"\"\n",
    "create_hypertable_wsb_comments_analytics = \"SELECT create_hypertable('wsb_comments_analytics', 'created_utc', 'comment_id', number_partitions := 10, migrate_data := TRUE);\"\n",
    "drop_wsb_comments_analytics_tickers_found = \"DROP TABLE IF EXISTS wsb_comments_analytics_tickers_found;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08285dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.904983Z",
     "start_time": "2021-07-17T19:50:01.789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prepared_statements_df.to_sql(\n",
    "    name=\"wsb_comments_analytics_smaller_temp\", \n",
    "    con=get_sqlalchemy_engine(),\n",
    "    if_exists=\"replace\", \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb3de9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Inserting using psycopg2 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f817d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.905983Z",
     "start_time": "2021-07-17T19:50:01.836Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx : min(ndx + n, l)]\n",
    "\n",
    "insert_statement = \\\n",
    "\"UPDATE wsb_comments_analytics SET tickers_found = string_to_array(%s, ', ') WHERE comment_id = %s;\"\n",
    "\n",
    "values = tuple(prepared_statements_df[[\"tickers\", \"comment_id\"]].to_records(index=False))\n",
    "\n",
    "all_mogrified_statements = []\n",
    "with get_conn() as conn: \n",
    "    with conn.cursor() as cur: \n",
    "        for i in tqdm(range(len(values))):\n",
    "            mogrified_update_stmt = cur.mogrify(insert_statement, (values[i][0], values[i][1]))\n",
    "            all_mogrified_statements.append(mogrified_update_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25aa47c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Uploading more ticker details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a54fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.906983Z",
     "start_time": "2021-07-17T19:50:01.880Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "companies_df = pd.read_csv(\"companies.csv\")\n",
    "companies_df[\"tags\"] = companies_df[\"tag 1\"] + \",\" + companies_df[\"tag 2\"] + \",\" + companies_df[\"tag 3\"]\n",
    "companies_df = companies_df.drop(columns=[\"tag 1\", \"tag 2\", \"tag 3\"])\n",
    "\n",
    "companies_df = companies_df.drop(columns=[\"short name\"])\n",
    "\n",
    "companies_df.columns = [\n",
    "    \"symbol\", \n",
    "    \"name\", \n",
    "    \"industry\", \n",
    "    \"description\", \n",
    "    \"url\", \n",
    "    \"logo\", \n",
    "    \"ceo\", \n",
    "    \"exchange\", \n",
    "    \"marketcap\", \n",
    "    \"sector\", \n",
    "    \"tags\"\n",
    "]\n",
    "\n",
    "cols_str = ', '.join(companies_df.columns.tolist())\n",
    "\n",
    "companies_df[\"tags\"] = companies_df.loc[~companies_df[\"tags\"].isna(), \"tags\"].apply(lambda x: json.dumps(x.split(\",\")))\n",
    "\n",
    "companies_df.loc[pd.isna(companies_df[\"tags\"]), \"tags\"] = json.dumps(\"[]\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur: \n",
    "        execute_values(cur, f\"INSERT INTO ticker_details({cols_str}) VALUES %s ON CONFLICT (symbol) DO NOTHING;\", tuple(companies_df.to_records(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661fed7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read all that again and find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c953d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.906983Z",
     "start_time": "2021-07-17T19:50:01.927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open(\"comm_id_ticker_info.json\", \"r\") as f: \n",
    "#     comm_id_ticker_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf431625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.907982Z",
     "start_time": "2021-07-17T19:50:01.929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_clean_eles = []\n",
    "for ele in tqdm(comm_id_ticker_info):\n",
    "    sub_id = list(ele.keys())[0]\n",
    "    all_clean_eles.append({ \"submission_id\": sub_id, \"sentence\": ele[sub_id][\"body\"], \"tickers\": [e[\"ticker\"] for e in ele[sub_id][\"tickers\"]] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6b2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.908983Z",
     "start_time": "2021-07-17T19:50:01.930Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_sents_tickers_df = pd.DataFrame.from_records(all_clean_eles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5ceb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.908983Z",
     "start_time": "2021-07-17T19:50:01.932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_sents_tickers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bffc30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T19:50:05.909982Z",
     "start_time": "2021-07-17T19:50:01.934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.merge(left=wsb_com_df, right=clean_sents_tickers_df[[\"submission_id\",\"tickers\"]], how=\"\", left_on=\"submission_id\", right_on=\"submission_id\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e373f1698d5c71273a51046ea3311cce7bbe56565c2a4a1b256ba6ae064c5d40"
  },
  "kernelspec": {
   "display_name": "py38-env",
   "language": "python",
   "name": "py38-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
